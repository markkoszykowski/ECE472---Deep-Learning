# -*- coding: utf-8 -*-
"""Assignment 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YXrqhBEAEgrZYf4Tgz3iHRC_qR4xxn9L
"""

## Mark Koszykowski
## ECE472 - Deep Learning
## Assignment 2

# My various attempts in building a functional form for the function f (which in my case is
# the percep function) went almost full circle. On the first try, my percep function took
# in the x values for one of the classes and utilized a similar loop to the current one in 
# order to create a prediction array of y values (from here I inferred that the loss
# function would be calculating the probabilities). After learning that it was necessary to 
# use all the data that was provided (as opposed to using one class of data and inferring
# that whatever wasn't in this class was in the other class) and that this function should
# ideally be outputing the probability for each set of points, the data structure that was
# fed into the function was rearranged however the crux of the function did not change much.
# The first major differences that were introduced were transposing the data so that the
# matrix multiplication could be properly executed and running the output array through a 
# sigmoid funciton to map the values to some probability. When this was done the output
# contour was still not what was desired, so I attempted a change in activation functions.
# In this third attempt, I used sigmoids as opposed to ReLUs since I had read in an online
# forum that sigmoids tend to be better for binary classification and the ReLUs often left
# me with nan outputs. This yielded slightly better results than the ReLUs at the time 
# however it required me to change many of my variables. When debugging other aspects of 
# the program, I was informed (by you) that ReLUs are a better route for this application, 
# especially since it seems that in general they can be applied in any deep learning 
# application. By this fouth attempt, I ended up with a function that was just about 
# exactly the same as my second attempt however this fourth attempt was its final form 
# and with it working reasonably well, the only other thing to do was adjust variables to 
# fine tune the weights.

# In the final output, the blue region denotes where the application is likely to suggest
# that a data point belongs to class 1 (the red data) and the white denotes the same for
# class 0 (the blue data). The boundary of these regions is where the application will make
# the classification into class 0 or 1 with 50% probability.

import numpy as np
import matplotlib.pyplot as plt
import jax
import jax.numpy as jnp
from jax import grad, vmap

from google.colab import files

# Define a function to set up spiral data set
def spiral_noise(t, rob):
    r = 39/2 * (104 - t)/104
    x = r * np.cos(np.pi * t * (1/16)) * rob
    y = r * np.sin(np.pi * t * (1/16)) * rob + np.random.normal(scale=0.2, size=200)
    return x, y

# Define a four layer perceptron function (f) which outputs probability array
def percep(data, params):
    data = np.transpose(data)

    for W, b in params:
        y_hat = jnp.dot(data, W) + b
        data = jax.nn.relu(y_hat)

    return jax.nn.sigmoid(jnp.squeeze(y_hat))
    
# Define a cost function which calculates the entropy with an L2 penalty
def cost(params, data, targ):
    y_hat = percep(data, params)

    bce = -1 * targ * jnp.log(y_hat) - (1 - targ) * jnp.log(1 - y_hat)

    l2 = 0.0
    for i in range(4):
          mat = params[i][0] ** 2
          l2 += sum(map(sum, mat))

    cost = sum(bce) + .0001 * l2

    return cost

# Define a step function which adjusts the parameters
@jax.jit
def step(data, targ, params, step_size):
    g = grad(cost)(params, data, targ)
    loss = cost(params, data, targ)

    for i in range(2):
        for j in range(4):
            params[j][i] = params[j][i] - step_size * g[j][i]

    return loss, params

# Quantify spiral data set
t = jnp.linspace(40, 96, 200)
x_r, y_r = spiral_noise(t, -1)
x_b, y_b = spiral_noise(t, 1)

# Arrange data in a 2 x 400 matrix
x = np.concatenate([x_r, x_b])
y = np.concatenate([y_r, y_b])
data = np.array([x, y])

# Define classes for each coordinate
targ = np.zeros(400)
for i in range(200):
  targ[i] = 1.0

# Initialize parameters
M = 40
step_size = .000001
params = [
          [np.random.normal(size=(2,M)), jnp.zeros(M)],
          [np.random.normal(size=(M,M)) / M, jnp.zeros(M)],
          [np.random.normal(size=(M,M)) / M, jnp.zeros(M)],
          [np.random.normal(size=(M,1)), jnp.zeros(1)],
]
epochs = 1000000

# Create data arrays for loss analysis over epochs (for debugging)
x_l = np.linspace(0, epochs, epochs)
l = np.zeros(epochs)

# Iterate through the step function to adjust params
for i in range(epochs):
     l[i], params = step(data, targ, params, step_size)

# Create 2D array of all points in 25 x 25 [X,Y c (-12.5,12.5)] square
b = np.zeros(2500)
temp = np.linspace(-12.5, 12.5, 50)
for i in range(50):
    if i == 0:
        a = np.linspace(-12.5, 12.5, 50)
    else:
        a = np.concatenate([a, temp])
    for j in range(50):
        b[i*50 + j] = -12.5 + i * (25/49)
fiftyfifty = np.array([a, b])

# Calculate probability of each point being in class 1 (red spiral)
probs = np.zeros(2500)
probs = percep(fiftyfifty, params)

# Setup data to send to contourf
x_50 = np.linspace(-12.5, 12.5, 50)
y_50 = np.linspace(-12.5, 12.5, 50)
probs = probs.reshape([50, 50])

# Create asked for graphs
fig = plt.figure()
plt.plot(x_r, y_r, 'r.', x_b, y_b, 'b.')
plt.contourf(x_50, y_50, probs, levels=[.5, 1], cmap="Blues")
plt.title("Spirals")
plt.xlabel("x")
plt.ylabel("y", rotation=0)
fig.show()
fig.savefig('Spirals.pdf')
files.download('Spirals.pdf')

